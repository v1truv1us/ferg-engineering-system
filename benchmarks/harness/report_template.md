# Validation Report Template

This template is used by the statistical analyzer to generate comprehensive validation reports.

## Template Variables

The following variables are available in the report generation:

- `{{title}}`: Report title
- `{{generated_at}}`: Generation timestamp
- `{{total_tasks}}`: Total number of tasks analyzed
- `{{significant_improvements}}`: Number of techniques with significant improvements
- `{{average_improvement}}`: Average improvement percentage across all techniques
- `{{technique_results}}`: Array of technique result objects
- `{{methodology}}`: Statistical methodology description

## Technique Result Object Structure

Each technique result contains:

```javascript
{
  technique: "expert-persona",
  sample_size: 30,
  improvement_percentage: 45.2,
  cohens_d: 1.5,
  wilcoxon_p_value: 0.0012,
  significant: true,
  confidence_interval: [-0.5, 2.5],
  power_analysis: {
    effect_size: 1.5,
    current_power: 0.95,
    required_sample_size: 20,
    sufficient_power: true
  }
}
```

## Full Report Structure

```markdown
# {{title}}
Generated: {{generated_at}}

## Executive Summary

- **Tasks Analyzed**: {{total_tasks}}
- **Significant Improvements**: {{significant_improvements}}
- **Average Improvement**: {{average_improvement}}%

## Technique Results

| Technique | Sample Size | Improvement | Cohen's d | p-value | Significant |
|-----------|-------------|-------------|-----------|---------|-------------|
{{#technique_results}}
| {{technique}} | {{sample_size}} | {{improvement_percentage}}% | {{cohens_d}} | {{wilcoxon_p_value}} | {{significant_marker}} |
{{/technique_results}}

## Detailed Analysis

{{#technique_results}}
### {{technique}}

#### Statistics
- **Sample Size**: {{sample_size}}
- **Baseline Mean**: {{mean_baseline}}
- **Enhanced Mean**: {{mean_enhanced}}
- **Mean Improvement**: {{mean_improvement}}
- **Improvement %**: {{improvement_percentage}}%

#### Effect Sizes
- **Cohen's d**: {{cohens_d}}
- **Hedges' g**: {{hedges_g}}

#### Statistical Tests
- **Wilcoxon Statistic**: {{wilcoxon_statistic}}
- **p-value**: {{wilcoxon_p_value}}
- **Significant**: {{significant_text}} (α = 0.05)

#### Confidence Interval (95%)
- **Lower Bound**: {{confidence_interval[0]}}
- **Upper Bound**: {{confidence_interval[1]}}

#### Power Analysis
- **Effect Size**: {{power_analysis.effect_size}}
- **Current Power**: {{power_analysis.current_power}}
- **Required Sample Size**: {{power_analysis.required_sample_size}}
- **Sufficient Power**: {{power_analysis.sufficient_power}}

{{/technique_results}}

## Methodology

### Statistical Tests
- **Wilcoxon Signed-Rank Test**: Paired, non-parametric test for related samples
- **Holm-Bonferroni Correction**: Multiple comparison correction for family-wise error
- **BCa Bootstrap**: Bias-corrected and accelerated confidence intervals

### Effect Size Measures
- **Cohen's d**: Standardized mean difference
- **Hedges' g**: Bias-corrected Cohen's d for small samples

### Significance Thresholds
- **α (Type I Error)**: 0.05
- **Power**: 0.80
- **Bootstrap Resamples**: 9999

## Claims Validation

Based on the experimental results, the following claims are:

### Validated Claims (p < 0.05)
{{#validated_claims}}
- **{{technique}}**: {{improvement_percentage}}% improvement (Claim: {{claimed_improvement}}%)
{{/validated_claims}}

### Unvalidated Claims (p ≥ 0.05)
{{#unvalidated_claims}}
- **{{technique}}**: {{improvement_percentage}}% improvement (Claim: {{claimed_improvement}}%)
{{/unvalidated_claims}}

## Recommendations

### For Implementation
1. **Prioritize Validated Techniques**: Focus on techniques that showed significant improvements
2. **Monitor Performance**: Track actual improvements in production use
3. **Iterate on Weak Techniques**: Investigate why some techniques underperformed

### For Future Research
1. **Increase Sample Size**: Larger samples for more precise effect size estimates
2. **Test Task Categories Separately**: Different techniques may work better for different task types
3. **Longitudinal Study**: Track improvements over time as models evolve

## Data Sources

- **Tasks**: 12 benchmark tasks across 4 categories
- **Techniques**: 5 prompting techniques tested
- **Samples**: {{total_samples}} total evaluations
- **Models**: Claude-3 and GPT-4 tested
- **Time Period**: {{date_range}}

---

*Report generated by Ferg Engineering System Validation Framework*
*Statistical analysis conducted with α=0.05, power=0.80*
*Confidence intervals calculated using BCa bootstrap (9999 resamples)*